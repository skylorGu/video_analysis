import os
import json
import tempfile
import warnings
import os
import re
import subprocess
import shutil
from flask import Flask, render_template, request, jsonify, send_file, url_for, redirect, session
from werkzeug.utils import secure_filename
from moviepy.editor import VideoFileClip, concatenate_videoclips
import sys
sys.path.append("../")
import numpy as np
from PIL import Image
import uuid

# å°è¯•å¯¼å…¥æ¨¡å‹ç›¸å…³åº“ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
try:
    import torch
    from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
    from omni_utils import process_mm_info  # Assuming this is a custom utility
    MODEL_AVAILABLE = True
except ImportError:
    MODEL_AVAILABLE = False
    print("Warning: Model libraries not available. Running in simulation mode.")

warnings.filterwarnings("ignore")

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'static/uploads'
app.config['CONVERSATIONS_FOLDER'] = 'conversations'
app.config['HIGHLIGHTS_FOLDER'] = 'static/highlights'
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024  # é™åˆ¶ä¸Šä¼ å¤§å°ä¸º500MB
app.config['SECRET_KEY'] = 'super_secret_key'  # ç”¨äºsession
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['CONVERSATIONS_FOLDER'], exist_ok=True)
os.makedirs(app.config['HIGHLIGHTS_FOLDER'], exist_ok=True)

# è®¾ç½®é™æ€æ–‡ä»¶ç¼“å­˜è¿‡æœŸæ—¶é—´ä¸º0ï¼Œé¿å…ç¼“å­˜é—®é¢˜
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0

# æ”¯æŒçš„è§†é¢‘æ ¼å¼
ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov', 'mkv'}

# é›†é”¦ç”Ÿæˆé…ç½®
SEGMENT_DURATION = 10  # æ¯ä¸ªåˆ‡ç‰‡çš„æ—¶é•¿ï¼ˆç§’ï¼‰
TOP_N_SEGMENTS = 3     # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„å‰Nä¸ªåˆ‡ç‰‡
USE_AUDIO_IN_VIDEO = True  # æ˜¯å¦åœ¨è§†é¢‘åˆ†æä¸­ä½¿ç”¨éŸ³é¢‘

def check_ffmpeg_available():
    """æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨"""
    try:
        result = subprocess.run(['ffmpeg', '-version'], 
                              capture_output=True, 
                              text=True, 
                              timeout=10)
        return result.returncode == 0
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
        return False

def check_moviepy_dependencies():
    """æ£€æŸ¥MoviePyçš„ä¾èµ–æ˜¯å¦å®Œæ•´"""
    try:
        from moviepy.editor import VideoFileClip
        # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•
        return True
    except ImportError as e:
        print(f"MoviePyå¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"MoviePyä¾èµ–æ£€æŸ¥å¤±è´¥: {e}")
        return False

# --- Model Initialization ---
# Load Qwen2.5-Omni model and processor
if MODEL_AVAILABLE:
    try:
        print("Loading Qwen2.5-Omni model and processor...")
        model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            "/data2/dyl/LLMCheckpoint/Qwen2.5-Omni-3B",
            torch_dtype=torch.bfloat16,
            device_map="auto",
            use_safetensors=True,
            attn_implementation="flash_attention_2",
        )
        model.disable_talker()  # As per your original code

        processor = Qwen2_5OmniProcessor.from_pretrained("/data2/dyl/LLMCheckpoint/Qwen2.5-Omni-3B")
        print("Model and processor loaded successfully.")
    except Exception as e:
        print(f"Error loading model: {e}")
        model = None
        processor = None
else:
    model = None
    processor = None

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_video_duration(video_path):
    """è·å–è§†é¢‘æ—¶é•¿"""
    try:
        with VideoFileClip(video_path) as clip:
            return clip.duration
    except Exception as e:
        print(f"è·å–è§†é¢‘æ—¶é•¿å¤±è´¥: {e}")
        return 0

def split_video_into_segments(video_path, segment_duration=SEGMENT_DURATION):
    """å°†è§†é¢‘åˆ†å‰²æˆæŒ‡å®šæ—¶é•¿çš„ç‰‡æ®µ"""
    try:
        duration = get_video_duration(video_path)
        if duration <= 0:
            return []
        
        segments = []
        current_time = 0
        
        while current_time < duration:
            end_time = min(current_time + segment_duration, duration)
            segments.append((current_time, end_time))
            current_time = end_time
        
        return segments
    except Exception as e:
        print(f"åˆ†å‰²è§†é¢‘å¤±è´¥: {e}")
        return []

def score_video_segment(video_path, start_time, end_time):
    """ä½¿ç”¨Qwenæ¨¡å‹ä¸ºè§†é¢‘ç‰‡æ®µæ‰“åˆ†"""
    if not MODEL_AVAILABLE or model is None or processor is None:
        # æ¨¡æ‹Ÿæ¨¡å¼ï¼šè¿”å›éšæœºåˆ†æ•°
        import random
        return random.randint(1, 10)
    
    try:
        # åˆ›å»ºä¸´æ—¶è§†é¢‘ç‰‡æ®µ
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:
            temp_path = temp_file.name
        
        try:
            with VideoFileClip(video_path) as clip:
                segment_clip = clip.subclip(start_time, end_time)
                segment_clip.write_videofile(temp_path, verbose=False, logger=None)
                segment_clip.close()
        except Exception as e:
            print(f"åˆ›å»ºä¸´æ—¶ç‰‡æ®µå¤±è´¥: {e}")
            return 0
        
        # ä½¿ç”¨æ¨¡å‹åˆ†æç‰‡æ®µ
        segment_conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "video", "video": temp_path},
                    {"type": "text", "text": "è¯·ä¸ºè¿™ä¸ªè§†é¢‘ç‰‡æ®µçš„ç²¾å½©ç¨‹åº¦æ‰“åˆ†ï¼Œåˆ†æ•°èŒƒå›´1-10åˆ†ï¼Œ10åˆ†æœ€ç²¾å½©ã€‚åªéœ€è¦å›å¤ï¼šè¯„åˆ†ï¼šXåˆ†"}
                ],
            },
        ]
        
        from qwen_omni_utils import process_mm_info
        text = processor.apply_chat_template(segment_conversation, add_generation_prompt=True, tokenize=False)
        audios, images, videos = process_mm_info(segment_conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
        inputs = processor(
            text=text,
            audio=audios,
            images=images,
            videos=videos,
            return_tensors="pt",
            padding=True,
            use_audio_in_video=USE_AUDIO_IN_VIDEO
        )
        
        inputs = inputs.to(model.device).to(model.dtype)
        
        text_ids = model.generate(
            **inputs,
            use_audio_in_video=USE_AUDIO_IN_VIDEO,
            return_audio=False,
            max_new_tokens=2048,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
            top_k=50,
            repetition_penalty=1.1
        )
        
        prompt_token_len = inputs['input_ids'].shape[-1]
        new_tokens = text_ids[0, prompt_token_len:]
        response = processor.tokenizer.decode(new_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)
        
        print(f"ğŸ¤– Qwen å¯¹ç‰‡æ®µ ({start_time:.1f}s-{end_time:.1f}s) çš„å›å¤: {response}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.unlink(temp_path)
        except:
            pass
        
        # æå–åˆ†æ•°
        match = re.search(r'[è¯„åˆ†P]åˆ†[:ï¼š]\s*(\d+)', response)
        if match:
            try:
                score = int(match.group(1))
                return score
            except ValueError:
                return 0
        return 0
        
    except Exception as e:
        print(f"å¯¹ç‰‡æ®µ ({start_time:.1f}s-{end_time:.1f}s) æ‰“åˆ†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return 0

def create_highlight_video(original_video_path, scored_segments, output_filename_base="highlight"):
    """æ ¹æ®è¯„åˆ†åˆ›å»ºé›†é”¦è§†é¢‘"""
    # æ£€æŸ¥ä¾èµ–
    if not check_ffmpeg_available():
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ffmpegï¼Œæ— æ³•è¿›è¡Œè§†é¢‘ç¼–ç ")
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
        print("   1. ä¸‹è½½å¹¶å®‰è£…ffmpeg: https://ffmpeg.org/download.html")
        print("   2. ç¡®ä¿ffmpegå·²æ·»åŠ åˆ°ç³»ç»ŸPATHç¯å¢ƒå˜é‡ä¸­")
        print("   3. é‡å¯åº”ç”¨ç¨‹åº")
        return None
    
    if not check_moviepy_dependencies():
        print("âŒ é”™è¯¯ï¼šMoviePyä¾èµ–ä¸å®Œæ•´")
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼špip install moviepy")
        return None
    
    if not scored_segments:
        print("æ²¡æœ‰å¯è¯„åˆ†çš„åˆ‡ç‰‡ï¼Œæ— æ³•åˆ›å»ºé›†é”¦ã€‚")
        return None
    
    # æŒ‰ç…§åˆ†æ•°é™åºæ’åˆ—ï¼Œå¹¶å–å‰Nä¸ª
    scored_segments.sort(key=lambda x: x[1], reverse=True)
    selected_segments_info = [s[0] for s in scored_segments[:TOP_N_SEGMENTS]]
    
    # æŒ‰ç…§åŸå§‹æ—¶é—´é¡ºåºé‡æ–°æ’åºï¼Œä»¥ä¿è¯é›†é”¦çš„é€»è¾‘æ€§
    selected_segments_info.sort(key=lambda x: x[0])  # x[0] æ˜¯å¼€å§‹æ—¶é—´
    
    if not selected_segments_info:
        print("æ²¡æœ‰è¶³å¤Ÿçš„åˆ‡ç‰‡æ¥åˆ›å»ºé›†é”¦ã€‚")
        return None
    
    try:
        full_video_clip = VideoFileClip(original_video_path)
    except Exception as e:
        print(f"åŠ è½½åŸå§‹è§†é¢‘ {original_video_path} å¤±è´¥: {e}")
        return None
    
    adjusted_clips = []
    last_segment_end_time = -1
    
    for start_time, end_time in selected_segments_info:
        current_segment_start_time = max(start_time, last_segment_end_time)
        
        if current_segment_start_time >= end_time:
            continue
        
        try:
            clip_to_add = full_video_clip.subclip(current_segment_start_time, end_time)
            adjusted_clips.append(clip_to_add)
            last_segment_end_time = end_time
        except Exception as e:
            print(f"æˆªå–è§†é¢‘ç‰‡æ®µ ({current_segment_start_time:.2f}s - {end_time:.2f}s) å¤±è´¥: {e}")
            continue
    
    if not adjusted_clips:
        print("æ²¡æœ‰å¯ç”¨äºåˆå¹¶çš„è°ƒæ•´ååˆ‡ç‰‡ã€‚")
        full_video_clip.close()
        return None
    
    output_filename = os.path.join(app.config['HIGHLIGHTS_FOLDER'], f"{output_filename_base}_{uuid.uuid4().hex}.mp4")
    
    try:
        print(f"ğŸ“¹ å¼€å§‹åˆå¹¶ {len(adjusted_clips)} ä¸ªè§†é¢‘ç‰‡æ®µ...")
        final_clip = concatenate_videoclips(adjusted_clips)
        
        print(f"ğŸ’¾ å¼€å§‹å†™å…¥é›†é”¦è§†é¢‘åˆ°: {output_filename}")
        print("âš ï¸ æ³¨æ„ï¼šè§†é¢‘ç¼–ç å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # æ·»åŠ æ›´è¯¦ç»†çš„å‚æ•°å’Œé”™è¯¯å¤„ç†
        final_clip.write_videofile(
            output_filename,
            codec="libx264",
            audio_codec="aac",
            verbose=True,  # æ”¹ä¸ºTrueä»¥æ˜¾ç¤ºè¿›åº¦
            logger='bar',  # æ˜¾ç¤ºè¿›åº¦æ¡
            temp_audiofile='temp-audio.m4a',
            remove_temp=True,
            fps=24,  # è®¾ç½®å›ºå®šå¸§ç‡
            preset='medium'  # ç¼–ç é¢„è®¾
        )
        
        print(f"âœ… é›†é”¦è§†é¢‘å†™å…¥å®Œæˆ: {output_filename}")
        
        final_clip.close()
        full_video_clip.close()
        
        # æ¸…ç†ä¸´æ—¶ç‰‡æ®µ
        for clip in adjusted_clips:
            clip.close()
        
        print(f"ğŸ‰ é›†é”¦è§†é¢‘åˆ›å»ºæˆåŠŸï¼Œæ–‡ä»¶å¤§å°: {os.path.getsize(output_filename) / (1024*1024):.2f} MB")
        return output_filename
        
    except Exception as e:
        print(f"âŒ åˆå¹¶è§†é¢‘å¤±è´¥: {e}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        
        # æ¸…ç†èµ„æº
        try:
            full_video_clip.close()
        except:
            pass
        
        for clip in adjusted_clips:
            try:
                clip.close()
            except:
                pass
        
        # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²éƒ¨åˆ†åˆ›å»ºï¼Œå°è¯•åˆ é™¤
        if os.path.exists(output_filename):
            try:
                os.remove(output_filename)
                print(f"ğŸ—‘ï¸ å·²æ¸…ç†éƒ¨åˆ†åˆ›å»ºçš„æ–‡ä»¶: {output_filename}")
            except:
                pass
        
        return None

def generate_response(conversation_history, use_audio_in_video=True):
    """Generates a text response from the Qwen-Omni model or simulation."""
    if not MODEL_AVAILABLE or model is None or processor is None:
        # æ¨¡æ‹Ÿæ¨¡å¼ï¼šè¿”å›åŸºäºå…³é”®è¯çš„ç®€å•å›å¤
        last_message = ""
        for msg in reversed(conversation_history):
            if msg.get('role') == 'user':
                for content in msg.get('content', []):
                    if content.get('type') == 'text':
                        last_message = content.get('text', '')
                        break
                break
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…å›å¤
        if 'æ‘˜è¦' in last_message or 'æ€»ç»“' in last_message:
            return "è¿™æ˜¯ä¸€ä¸ªå…³äºAIæŠ€æœ¯çš„è§†é¢‘ï¼Œä¸»è¦ä»‹ç»äº†æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æ¦‚å¿µï¼Œä»¥åŠå®ƒä»¬åœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨ã€‚"
        elif 'æ€ç»´å¯¼å›¾' in last_message or 'mindmap' in last_message.lower():
            return "# è§†é¢‘å†…å®¹æ€ç»´å¯¼å›¾\n\n## AIæŠ€æœ¯ä»‹ç»\n- æœºå™¨å­¦ä¹ åŸºç¡€\n- æ·±åº¦å­¦ä¹ æ¦‚å¿µ\n\n## åº”ç”¨åœºæ™¯\n- å›¾åƒè¯†åˆ«\n- è‡ªç„¶è¯­è¨€å¤„ç†"
        else:
            return f"åŸºäºæ‚¨çš„é—®é¢˜ï¼š{last_message}ï¼Œè¿™ä¸ªè§†é¢‘ä¸»è¦è®¨è®ºäº†AIç›¸å…³æŠ€æœ¯ã€‚å¦‚éœ€æ›´è¯¦ç»†çš„åˆ†æï¼Œè¯·ç¡®ä¿æ¨¡å‹åº“å·²æ­£ç¡®å®‰è£…ã€‚"

    try:
        text = processor.apply_chat_template(conversation_history, add_generation_prompt=True, tokenize=False)
        audios, images, videos = process_mm_info(conversation_history, use_audio_in_video=use_audio_in_video)
        
        inputs = processor(
            text=text,
            audio=audios,
            images=images,
            videos=videos,
            return_tensors="pt",
            padding=True,
            use_audio_in_video=use_audio_in_video
        )
        inputs = inputs.to(model.device).to(model.dtype)

        text_ids = model.generate(
            **inputs,
            use_audio_in_video=use_audio_in_video,
            return_audio=False,
            max_new_tokens=2048,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
            top_k=50,
            repetition_penalty=1.1
        )
        prompt_token_len = inputs['input_ids'].shape[-1]

        new_tokens = text_ids[0, prompt_token_len:]  # æˆªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        response = processor.tokenizer.decode(new_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)
        return response
    except Exception as e:
        print(f"Error during model generation: {e}")
        return "An error occurred during response generation."

def get_conversation_file_path(session_id):
    return os.path.join(app.config['CONVERSATIONS_FOLDER'], f"{session_id}.json")

def load_conversation(session_id):
    file_path = get_conversation_file_path(session_id)
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def save_conversation(session_id, conversation):
    file_path = get_conversation_file_path(session_id)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(conversation, f, ensure_ascii=False, indent=4)

def process_video_with_qwen(video_path):
    """Process video using Qwen2.5-Omni model"""
    if not MODEL_AVAILABLE or model is None or processor is None:
        # æ¨¡æ‹Ÿæ¨¡å¼ï¼šè¿”å›ç¤ºä¾‹æ•°æ®
        demo_summary = """# ğŸ“¹ è§†é¢‘å†…å®¹æ‘˜è¦

## ğŸ¯ æ¦‚è¿°
è¿™æ˜¯ä¸€ä¸ªå…³äº**AIæŠ€æœ¯**çš„æ•™è‚²è§†é¢‘ã€‚è§†é¢‘å†…å®¹æ·±å…¥ä»‹ç»äº†æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µï¼Œæ¶µç›–äº†ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨çš„å®Œæ•´çŸ¥è¯†ä½“ç³»ã€‚

## ğŸ“Š è§†é¢‘ç»Ÿè®¡
- **æ—¶é•¿**: 35åˆ†é’Ÿ
- **ä¸»è¦æ¦‚å¿µ**: 6ä¸ª
- **åº”ç”¨æ¡ˆä¾‹**: 3ä¸ª
- **æŠ€æœ¯æ·±åº¦**: ä¸­çº§

## ğŸ”‘ æ ¸å¿ƒè¦ç‚¹

### 1. æœºå™¨å­¦ä¹ åŸºç¡€
- **ç›‘ç£å­¦ä¹ **: ä½¿ç”¨æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒçš„å­¦ä¹ æ–¹æ³•
- **æ— ç›‘ç£å­¦ä¹ **: ä»æœªæ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼çš„æŠ€æœ¯
- **å¼ºåŒ–å­¦ä¹ **: é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥

### 2. æ·±åº¦å­¦ä¹ æŠ€æœ¯
- **ç¥ç»ç½‘ç»œ**: æ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒè¿æ¥çš„è®¡ç®—æ¨¡å‹
- **å·ç§¯ç¥ç»ç½‘ç»œ**: ä¸“é—¨ç”¨äºå›¾åƒå¤„ç†çš„æ·±åº¦å­¦ä¹ æ¶æ„
- **å¾ªç¯ç¥ç»ç½‘ç»œ**: å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œç»“æ„

### 3. å®é™…åº”ç”¨é¢†åŸŸ
- **å›¾åƒè¯†åˆ«**: è®¡ç®—æœºè§†è§‰ä¸­çš„æ ¸å¿ƒåº”ç”¨
- **è‡ªç„¶è¯­è¨€å¤„ç†**: ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€çš„æŠ€æœ¯
- **è¯­éŸ³è¯†åˆ«**: å°†è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºæ–‡æœ¬çš„æŠ€æœ¯

## ğŸ’¡ é‡ç‚¹æç¤º
> ğŸš€ **å…³é”®æ´å¯Ÿ**: è§†é¢‘å¼ºè°ƒäº†ç†è®ºä¸å®è·µç›¸ç»“åˆçš„é‡è¦æ€§ï¼Œå±•ç¤ºäº†AIæŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚

## ğŸ·ï¸ å†…å®¹æ ‡ç­¾
`ğŸ¤– æœºå™¨å­¦ä¹ ` `ğŸ§  æ·±åº¦å­¦ä¹ ` `ğŸ”¬ æŠ€æœ¯æ•™è‚²` `ğŸ’¼ å®é™…åº”ç”¨`"""
        
        return {
            "summary": demo_summary,
            "mindmap": {
                "root": {
                    "text": "AIæŠ€æœ¯è§†é¢‘å†…å®¹",
                    "children": [
                        {
                            "text": "æœºå™¨å­¦ä¹ åŸºç¡€",
                            "children": [
                                {"text": "ç›‘ç£å­¦ä¹ "},
                                {"text": "æ— ç›‘ç£å­¦ä¹ "},
                                {"text": "å¼ºåŒ–å­¦ä¹ "}
                            ]
                        },
                        {
                            "text": "æ·±åº¦å­¦ä¹ æŠ€æœ¯",
                            "children": [
                                {"text": "ç¥ç»ç½‘ç»œ"},
                                {"text": "å·ç§¯ç¥ç»ç½‘ç»œ"},
                                {"text": "å¾ªç¯ç¥ç»ç½‘ç»œ"}
                            ]
                        },
                        {
                            "text": "åº”ç”¨é¢†åŸŸ",
                            "children": [
                                {"text": "å›¾åƒè¯†åˆ«"},
                                {"text": "è‡ªç„¶è¯­è¨€å¤„ç†"},
                                {"text": "è¯­éŸ³è¯†åˆ«"}
                            ]
                        }
                    ]
                }
            }
        }
    
    try:
        # Create conversation for video analysis
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "video", "video": video_path},
                    {"type": "text", "text": "è¯·è¯¦ç»†åˆ†æè¿™ä¸ªè§†é¢‘çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦ä¸»é¢˜ã€å…³é”®ä¿¡æ¯å’Œé‡è¦è§‚ç‚¹ã€‚"}
                ],
            },
        ]
        
        summary = generate_response(conversation, use_audio_in_video=True)
        
        # Generate mindmap based on summary
        mindmap_conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": f"åŸºäºä»¥ä¸‹è§†é¢‘åˆ†æå†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„æ€ç»´å¯¼å›¾JSONæ ¼å¼ï¼š\n{summary}\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š{{\"root\": {{\"text\": \"ä¸­å¿ƒä¸»é¢˜\", \"children\": [{{\"text\": \"åˆ†æ”¯1\", \"children\": [{{\"text\": \"å­åˆ†æ”¯1\"}}, {{\"text\": \"å­åˆ†æ”¯2\"}}]}}, {{\"text\": \"åˆ†æ”¯2\", \"children\": [{{\"text\": \"å­åˆ†æ”¯3\"}}, {{\"text\": \"å­åˆ†æ”¯4\"}}]}}]}}}}"}
                ],
            },
        ]
        
        mindmap_response = generate_response(mindmap_conversation, use_audio_in_video=True)
        
        # Try to parse mindmap JSON
        try:
            # Clean the response to extract JSON
            mindmap_response = mindmap_response.strip()
            if mindmap_response.startswith('```json'):
                mindmap_response = mindmap_response[7:]
            if mindmap_response.endswith('```'):
                mindmap_response = mindmap_response[:-3]
            mindmap_response = mindmap_response.strip()
            
            mindmap_data = json.loads(mindmap_response)
        except json.JSONDecodeError:
            # Fallback mindmap if JSON parsing fails
            mindmap_data = {
                "root": {
                    "text": "è§†é¢‘å†…å®¹åˆ†æ",
                    "children": [
                        {
                            "text": "ä¸»è¦å†…å®¹",
                            "children": [
                                {"text": "æ ¸å¿ƒä¸»é¢˜"},
                                {"text": "å…³é”®ä¿¡æ¯"}
                            ]
                        },
                        {
                            "text": "é‡è¦è§‚ç‚¹",
                            "children": [
                                {"text": "è§‚ç‚¹1"},
                                {"text": "è§‚ç‚¹2"}
                            ]
                        }
                    ]
                }
            }
        
        return {
            "summary": summary,
            "mindmap": mindmap_data
        }
        
    except Exception as e:
        print(f"Error processing video: {e}")
        return {
            "summary": f"è§†é¢‘å¤„ç†å‡ºé”™: {str(e)}",
            "mindmap": {"root": {"text": "é”™è¯¯", "children": []}}
        }



@app.route('/')
def index():
    return render_template('index.html')

@app.route('/result')
def result():
    from flask import session
    
    # æ£€æŸ¥sessionä¸­æ˜¯å¦æœ‰åˆ†æç»“æœ
    if 'analysis_result' not in session:
        return redirect(url_for('index'))
    
    result_data = session['analysis_result']
    
    # æ„å»ºè§†é¢‘URL
    video_url = url_for('serve_video', filename=result_data['video_filename'])
    
    return render_template('result.html',
                         summary=result_data['summary'],
                         mindmap_data=result_data['mindmap'],
                         video_url=video_url)

@app.route('/highlights')
def highlights():
    from flask import session
    
    # æ£€æŸ¥sessionä¸­æ˜¯å¦æœ‰åˆ†æç»“æœ
    if 'analysis_result' not in session:
        return redirect(url_for('index'))
    
    result_data = session['analysis_result']
    
    # æ„å»ºè§†é¢‘URL
    video_url = url_for('serve_video', filename=result_data['video_filename'])
    
    return render_template('highlights.html', video_url=video_url)

@app.route('/upload', methods=['POST'])
def upload_file():
    if 'video' not in request.files:
        return jsonify({"error": "æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"}), 400
    
    file = request.files['video']
    
    if file.filename == '':
        return jsonify({"error": "æœªé€‰æ‹©æ–‡ä»¶"}), 400
    
    if file and allowed_file(file.filename):
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        
        # åˆå§‹åŒ–session
        if 'session_id' not in session:
            session['session_id'] = os.urandom(16).hex()
        
        session_id = session['session_id']
        
        # ä½¿ç”¨çœŸå®çš„æ¨¡å‹å¤„ç†è§†é¢‘
        try:
            result = process_video_with_qwen(filepath)
            
            # åˆå§‹åŒ–å¯¹è¯å†å²
            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "video", "video": filepath},
                    ],
                },
            ]
            save_conversation(session_id, conversation)
            
            session['analysis_result'] = {
                'summary': result['summary'],
                'mindmap': result['mindmap'],
                'video_filename': filename
            }
            
            return jsonify({
                "status": "success",
                "redirect_url": "/result"
            })
            
        except Exception as e:
            print(f"Error processing video: {e}")
            return jsonify({"error": f"è§†é¢‘å¤„ç†å¤±è´¥: {str(e)}"}), 500
    
    return jsonify({"error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"}), 400

@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    question = data.get('question', '')
    use_audio = data.get('use_audio', True)
    
    session_id = session.get('session_id')
    
    if not session_id:
        return jsonify({'error': 'Session not initialized. Please upload a video first.'}), 400

    if not question:
        return jsonify({'error': 'No message provided'}), 400

    try:
        conversation = load_conversation(session_id)
        
        # Add user message to conversation history
        conversation.append({
            "role": "user",
            "content": [{"type": "text", "text": question}],
        })

        # Generate response using the model
        response_text = generate_response(conversation, use_audio_in_video=use_audio)

        # Add assistant response to conversation history
        conversation.append({
            "role": "assistant",
            "content": [{"type": "text", "text": response_text}],
        })

        save_conversation(session_id, conversation)

        return jsonify({
            "answer": response_text
        })
        
    except Exception as e:
        print(f"Error in chat: {e}")
        return jsonify({"error": f"å¯¹è¯å¤„ç†å¤±è´¥: {str(e)}"}), 500

@app.route('/summarize_video', methods=['POST'])
def summarize_video():
    """ç”Ÿæˆè§†é¢‘æ‘˜è¦"""
    session_id = session.get('session_id')
    
    if not session_id:
        return jsonify({'error': 'Session not initialized. Please upload a video first.'}), 400
    
    try:
        conversation = load_conversation(session_id)
        
        # åˆ›å»ºä¸´æ—¶å¯¹è¯ç”¨äºç”Ÿæˆæ‘˜è¦
        temp_conversation = conversation[:1]  # åªåŒ…å«è§†é¢‘ä¸Šä¸‹æ–‡
        temp_conversation.append({
            "role": "user",
            "content": [{"type": "text", "text": "è¯·ä¸ºè¿™ä¸ªè§†é¢‘ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„Markdownæ ¼å¼æ‘˜è¦ï¼Œè¦æ±‚ï¼š\n1. ä½¿ç”¨æ ‡é¢˜å’Œå­æ ‡é¢˜ç»„ç»‡å†…å®¹ç»“æ„\n2. ç”¨åˆ—è¡¨å±•ç¤ºå…³é”®ç‚¹\n3. çªå‡ºé‡è¦ä¿¡æ¯\n4. åŒ…å«è§†é¢‘çš„ä¸»è¦å†…å®¹ã€æ ¸å¿ƒæ¦‚å¿µå’Œå®è·µè¦ç‚¹\n5. ä½¿ç”¨é€‚å½“çš„Markdownè¯­æ³•ï¼ˆå¦‚**ç²—ä½“**ã€*æ–œä½“*ã€- åˆ—è¡¨ç­‰ï¼‰"}],
        })
        
        # ç”Ÿæˆæ‘˜è¦
        summary = generate_response(temp_conversation)
        
        # å¯é€‰ï¼šå°†æ‘˜è¦è¯·æ±‚å’Œå“åº”ä¿å­˜åˆ°ä¸»å¯¹è¯å†å²
        save_to_main = request.json.get('save_to_main', False) if request.json else False
        if save_to_main:
            conversation.extend(temp_conversation[1:])  # æ·»åŠ ç”¨æˆ·è¯·æ±‚
            conversation.append({
                "role": "assistant",
                "content": [{"type": "text", "text": summary}],
            })
            save_conversation(session_id, conversation)
        
        return jsonify({'summary': summary})
        
    except Exception as e:
        print(f"Error generating summary: {e}")
        return jsonify({'error': f'æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}'}), 500

@app.route('/generate_mindmap', methods=['POST'])
def generate_mindmap():
    """ç”Ÿæˆæ€ç»´å¯¼å›¾"""
    session_id = session.get('session_id')
    
    if not session_id:
        return jsonify({'error': 'Session not initialized. Please upload a video first.'}), 400
    
    try:
        conversation = load_conversation(session_id)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘åœ¨å½“å‰å¯¹è¯ä¸­
        if not conversation or not any(msg.get('role') == 'user' and 
                                     any(content.get('type') == 'video' for content in msg.get('content', [])) 
                                     for msg in conversation):
            return jsonify({'error': 'No video found in current conversation'}), 400
        
        # åˆ›å»ºä¸´æ—¶å¯¹è¯ç”¨äºç”Ÿæˆæ€ç»´å¯¼å›¾
        temp_conversation = conversation[:1]  # åªåŒ…å«è§†é¢‘ä¸Šä¸‹æ–‡
        temp_conversation.append({
            "role": "user",
            "content": [{"type": "text", "text": "è¯·ä¸ºè¿™ä¸ªè§†é¢‘ç”Ÿæˆä¸€ä¸ªMarkdownæ ¼å¼çš„æ€ç»´å¯¼å›¾ï¼ŒåŒ…å«ä¸»è¦ä¸»é¢˜å’Œå­ä¸»é¢˜çš„å±‚æ¬¡ç»“æ„ã€‚"}],
        })
        
        # ç”Ÿæˆæ€ç»´å¯¼å›¾
        mindmap_text = generate_response(temp_conversation)
        
        # å¯é€‰ï¼šå°†æ€ç»´å¯¼å›¾è¯·æ±‚å’Œå“åº”ä¿å­˜åˆ°ä¸»å¯¹è¯å†å²
        save_to_main = request.json.get('save_to_main', False) if request.json else False
        if save_to_main:
            conversation.extend(temp_conversation[1:])  # æ·»åŠ ç”¨æˆ·è¯·æ±‚
            conversation.append({
                "role": "assistant",
                "content": [{"type": "text", "text": mindmap_text}],
            })
            save_conversation(session_id, conversation)
        
        return jsonify({'mindmap': mindmap_text})
        
    except Exception as e:
        print(f"Error generating mindmap: {e}")
        return jsonify({'error': f'æ€ç»´å¯¼å›¾ç”Ÿæˆå¤±è´¥: {str(e)}'}), 500

@app.route('/template-data', methods=['GET'])
def template_data():
    """æä¾›æ¨¡æ¿æ•°æ®ï¼Œæ— éœ€ä¸Šä¼ è§†é¢‘"""
    # åˆ›å»ºç¤ºä¾‹æ€ç»´å¯¼å›¾æ•°æ®
    mindmap_data = {
        "root": {
            "text": "ç¤ºä¾‹æ€ç»´å¯¼å›¾",
            "children": [
                {
                    "text": "ä¸»é¢˜1",
                    "children": [
                        {"text": "å­ä¸»é¢˜1.1"},
                        {"text": "å­ä¸»é¢˜1.2"}
                    ]
                },
                {
                    "text": "ä¸»é¢˜2",
                    "children": [
                        {"text": "å­ä¸»é¢˜2.1"},
                        {"text": "å­ä¸»é¢˜2.2"}
                    ]
                }
            ]
        }
    }
    
    # åˆå§‹åŒ–session
    if 'session_id' not in session:
        session['session_id'] = os.urandom(16).hex()
    
    # åˆ›å»ºç¤ºä¾‹Markdownæ ¼å¼æ‘˜è¦
    demo_summary = """# ğŸ“¹ è§†é¢‘å†…å®¹æ‘˜è¦

## ğŸ¯ æ¦‚è¿°
è¿™ä¸ªè§†é¢‘æ·±å…¥ä»‹ç»äº†**ChatGPT Promptå·¥ç¨‹**çš„æ ¸å¿ƒæ¦‚å¿µå’Œå®è·µæŠ€å·§ï¼Œä¸»è¦ç”±*OpenAIçš„Isa Fulford*å’Œ*DeepLearning.AIçš„Andrew Ng*è”åˆè®²è§£ã€‚è¯¾ç¨‹å†…å®¹æ¶µç›–äº†ä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§åº”ç”¨çš„å®Œæ•´çŸ¥è¯†ä½“ç³»ã€‚

## ğŸ“Š è§†é¢‘ç»Ÿè®¡
- **æ—¶é•¿**: 45åˆ†é’Ÿ
- **æ ¸å¿ƒæ¦‚å¿µ**: 8ä¸ª
- **å®è·µæŠ€å·§**: 15ä¸ª
- **å†…å®¹è¦†ç›–**: 95%

## ğŸ”‘ æ ¸å¿ƒè¦ç‚¹

### 1. LLM APIåº”ç”¨
- å¦‚ä½•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹APIæ„å»ºæ™ºèƒ½è½¯ä»¶åº”ç”¨
- æ¥å£è°ƒç”¨å’Œå‚æ•°ä¼˜åŒ–æŠ€å·§
- å®é™…é¡¹ç›®ä¸­çš„åº”ç”¨åœºæ™¯

### 2. Promptè®¾è®¡åŸåˆ™
- æŒæ¡æç¤ºè¯å·¥ç¨‹çš„é‡è¦æ€§
- å­¦ä¹ ç¼–å†™**é«˜æ•ˆæç¤ºè¯**çš„æŠ€å·§
- é¿å…å¸¸è§çš„è®¾è®¡è¯¯åŒº

### 3. æ¨¡å‹ç‰¹æ€§ç†è§£
- æ·±å…¥äº†è§£ChatGPTæ¨¡å‹çš„ç‰¹æ€§
- ä¸åŒåœºæ™¯ä¸‹çš„é€‚ç”¨æ€§åˆ†æ
- æ¨¡å‹å±€é™æ€§å’Œä½¿ç”¨æ³¨æ„äº‹é¡¹

### 4. ä¼˜åŒ–ç­–ç•¥
- å¦‚ä½•ä¼˜åŒ–æç¤ºè¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœ
- æå‡æ¨¡å‹è¾“å‡ºè´¨é‡çš„æ–¹æ³•
- è¿­ä»£æ”¹è¿›çš„æœ€ä½³å®è·µ

### 5. å®‰å…¨è€ƒé‡
- AIå®‰å…¨çš„é‡è¦æ€§
- å¦‚ä½•é˜²èŒƒæ½œåœ¨é£é™©å’Œè¯¯ç”¨
- è´Ÿè´£ä»»çš„AIä½¿ç”¨åŸåˆ™

## ğŸ’¡ é‡ç‚¹æç¤º
> ğŸ’« **å…³é”®æ´å¯Ÿ**: è®²è€…å¼ºè°ƒè‰¯å¥½çš„promptè®¾è®¡å¯ä»¥æ˜¾è‘—æé«˜LLMçš„æ•ˆç‡å’Œå®‰å…¨æ€§ï¼Œè¿™æ˜¯æ„å»ºå¯é AIåº”ç”¨çš„å…³é”®åŸºç¡€ã€‚

## ğŸ·ï¸ å†…å®¹æ ‡ç­¾
`ğŸ¤– AIæŠ€æœ¯` `ğŸ’¡ Promptå·¥ç¨‹` `ğŸ”§ å®è·µåº”ç”¨` `ğŸ“š æ•™ç¨‹æŒ‡å—`"""
    
    # å°†ç»“æœå­˜å‚¨åˆ°sessionä¸­
    session['analysis_result'] = {
        'summary': demo_summary,
        'mindmap': mindmap_data,
        'video_filename': 'template_video.mp4'  # æ¨¡æ‹Ÿè§†é¢‘æ–‡ä»¶å
    }
    
    # è¿”å›ç®€å•çš„ç¤ºä¾‹æ•°æ®
    return jsonify({
        "status": "success",
        "summary": demo_summary,
        "mindmap": mindmap_data
    })



@app.route('/video/<filename>')
def serve_video(filename):
    """ç›´æ¥æä¾›è§†é¢‘æ–‡ä»¶çš„è®¿é—®"""
    video_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    if not os.path.exists(video_path):
        return jsonify({"error": "è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨"}), 404
    
    # è¿”å›è§†é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨åˆé€‚çš„MIMEç±»å‹
    if filename.lower().endswith('.mp4'):
        mimetype = 'video/mp4'
    elif filename.lower().endswith('.avi'):
        mimetype = 'video/x-msvideo'
    elif filename.lower().endswith('.mov'):
        mimetype = 'video/quicktime'
    else:
        mimetype = 'application/octet-stream'
    
    return send_file(video_path, mimetype=mimetype)

@app.route('/fixed-video')
def serve_fixed_video():
    """æä¾›å›ºå®šçš„è§†é¢‘æ–‡ä»¶"""
    fixed_video = "test2.mp4"
    video_path = os.path.join(app.config['UPLOAD_FOLDER'], fixed_video)
    
    if not os.path.exists(video_path):
        return jsonify({"error": "å›ºå®šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨"}), 404
    
    return send_file(video_path, mimetype='video/mp4')

@app.route('/generate_highlights', methods=['POST'])
def generate_highlights():
    """ç”Ÿæˆè§†é¢‘é›†é”¦"""
    session_id = session.get('session_id')
    
    if not session_id or 'analysis_result' not in session:
        return jsonify({'error': 'è¯·å…ˆä¸Šä¼ è§†é¢‘è¿›è¡Œåˆ†æ'}), 400
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹å¯ç”¨
        if not MODEL_AVAILABLE or model is None or processor is None:
            print("ğŸ­ æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨ç¤ºä¾‹é›†é”¦è§†é¢‘")
            
            # ä½¿ç”¨åŸè§†é¢‘ä½œä¸ºç¤ºä¾‹é›†é”¦ï¼ˆæ¨¡æ‹Ÿé›†é”¦æ•ˆæœï¼‰
            result_data = session['analysis_result']
            video_filename = result_data['video_filename']
            original_video_path = os.path.join(app.config['UPLOAD_FOLDER'], video_filename)
            
            if not os.path.exists(original_video_path):
                return jsonify({'error': 'åŸè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨'}), 404
            
            # ç›´æ¥ä½¿ç”¨åŸè§†é¢‘ä½œä¸ºé›†é”¦è§†é¢‘çš„URL
            highlight_url = url_for('serve_video', filename=video_filename)
            
            print(f"âœ… ç¤ºä¾‹é›†é”¦è§†é¢‘å‡†å¤‡å®Œæˆ: {video_filename}")
            
            return jsonify({
                'status': 'success',
                'highlight_url': highlight_url,
                'highlight_filename': f"highlight_{video_filename}",
                'message': 'é›†é”¦è§†é¢‘ç”ŸæˆæˆåŠŸï¼'
            })
        
        # è·å–åŸè§†é¢‘æ–‡ä»¶è·¯å¾„
        result_data = session['analysis_result']
        video_filename = result_data['video_filename']
        original_video_path = os.path.join(app.config['UPLOAD_FOLDER'], video_filename)
        
        if not os.path.exists(original_video_path):
            return jsonify({'error': 'åŸè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # åˆ†å‰²è§†é¢‘ä¸ºç‰‡æ®µ
        print(f"ğŸ¬ å¼€å§‹åˆ†å‰²è§†é¢‘: {original_video_path}")
        segments = split_video_into_segments(original_video_path)
        
        if not segments:
            return jsonify({'error': 'è§†é¢‘åˆ†å‰²å¤±è´¥'}), 500
        
        print(f"ğŸ“Š è§†é¢‘å·²åˆ†å‰²ä¸º {len(segments)} ä¸ªç‰‡æ®µ")
        
        # ä¸ºæ¯ä¸ªç‰‡æ®µæ‰“åˆ†
        scored_segments = []
        total_segments = len(segments)
        
        for i, (start_time, end_time) in enumerate(segments):
            print(f"ğŸ” æ­£åœ¨åˆ†æç‰‡æ®µ {i+1}/{total_segments}: {start_time:.1f}s - {end_time:.1f}s")
            score = score_video_segment(original_video_path, start_time, end_time)
            scored_segments.append(((start_time, end_time), score))
            print(f"â­ ç‰‡æ®µ {i+1} å¾—åˆ†: {score}")
        
        # åˆ›å»ºé›†é”¦è§†é¢‘
        print("ğŸ¥ å¼€å§‹åˆ›å»ºé›†é”¦è§†é¢‘...")
        print("ğŸ”§ æ­£åœ¨æ£€æŸ¥è§†é¢‘ç¼–ç ä¾èµ–...")
        
        highlight_video_path = create_highlight_video(original_video_path, scored_segments)
        
        if not highlight_video_path:
            error_msg = "é›†é”¦è§†é¢‘åˆ›å»ºå¤±è´¥ã€‚å¯èƒ½çš„åŸå› ï¼š\n"
            error_msg += "1. ffmpegæœªå®‰è£…æˆ–æœªæ·»åŠ åˆ°PATHç¯å¢ƒå˜é‡\n"
            error_msg += "2. MoviePyä¾èµ–ä¸å®Œæ•´\n"
            error_msg += "3. è§†é¢‘æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ\n"
            error_msg += "4. ç£ç›˜ç©ºé—´ä¸è¶³\n"
            error_msg += "è¯·æ£€æŸ¥æ§åˆ¶å°è¾“å‡ºè·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚"
            print(f"âŒ {error_msg}")
            return jsonify({'error': error_msg}), 500
        
        # è·å–é›†é”¦è§†é¢‘çš„ç›¸å¯¹è·¯å¾„ç”¨äºå‰ç«¯è®¿é—®
        highlight_filename = os.path.basename(highlight_video_path)
        highlight_url = url_for('serve_highlight_video', filename=highlight_filename)
        
        print(f"âœ… é›†é”¦è§†é¢‘åˆ›å»ºæˆåŠŸ: {highlight_video_path}")
        
        return jsonify({
            'status': 'success',
            'highlight_url': highlight_url,
            'highlight_filename': highlight_filename,
            'message': 'é›†é”¦è§†é¢‘ç”ŸæˆæˆåŠŸï¼'
        })
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆé›†é”¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'error': f'é›†é”¦ç”Ÿæˆå¤±è´¥: {str(e)}'}), 500

@app.route('/highlights/<filename>')
def serve_highlight_video(filename):
    """æä¾›é›†é”¦è§†é¢‘æ–‡ä»¶çš„è®¿é—®"""
    video_path = os.path.join(app.config['HIGHLIGHTS_FOLDER'], filename)
    if not os.path.exists(video_path):
        return jsonify({"error": "é›†é”¦è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨"}), 404
    
    return send_file(video_path, mimetype='video/mp4')

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000, use_reloader=False) # å°† use_reloader è®¾ç½®ä¸º False
